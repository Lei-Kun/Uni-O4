env                  : walker2d-medium-replay-v2 
seed                 : 0 
gpu                  : 5 
log_freq             : 2000 
path                 : logsshuffle 
v_steps              : 500000 
v_hidden_dim         : 512 
v_depth              : 3 
v_lr                 : 0.0001 
v_batch_size         : 512 
q_bc_steps           : 500000 
q_pi_steps           : 10 
q_hidden_dim         : 1024 
q_depth              : 2 
q_lr                 : 0.0001 
q_batch_size         : 512 
target_update_freq   : 2 
is_offpolicy_update  : False 
bc_steps             : 400000 
save_num             : 4 
bc_hidden_dim        : 256 
bc_depth             : 3 
bc_lr                : 0.0001 
bc_batch_size        : 512 
pi_activation_f      : relu 
is_filter_bc         : False 
alpha_bc             : 0.1 
num_policies         : 4 
bppo_steps           : 10000 
bppo_hidden_dim      : 256 
bppo_depth           : 3 
bppo_lr              : 0.0001 
bppo_batch_size      : 512 
clip_ratio           : 0.25 
entropy_weight       : 0.0 
decay                : 0.96 
omega                : 0.7 
is_clip_decay        : True 
is_bppo_lr_decay     : False 
is_update_old_policy : True 
is_state_norm        : True 
is_eval_state_norm   : False 
is_linear_decay      : True 
temperature          : None 
is_iql               : True 
is_double_q          : True 
is_shuffle           : True 
percentage           : 1.0 
eval_step            : 100 
task                 : hopper-medium-v2 
algo_name            : mobile 
rollout_step         : 1000 
kl_bc                : data 
kl_type              : heuristic 
is_kl_update         : False 
kl_strategy          : max 
alpha_bppo           : 0.1 
scale_strategy       : None 
eval_freq            : 500 
is_clip_action       : False 
eval_episode         : 10 
domain               : gym 
actor_lr             : 0.0001 
critic_lr            : 0.0003 
hidden_dims          : [256, 256] 
gamma                : 0.99 
tau                  : 0.005 
alpha                : 0.1 
auto_alpha           : True 
target_entropy       : None 
alpha_lr             : 0.0001 
deterministic_backup : False 
max_q_backup         : False 
norm_reward          : False 
num_q_ensemble       : 2 
dynamics_lr          : 0.001 
dynamics_max_epochs  : None 
max_epochs_since_update : 5 
dynamics_hidden_dims : [200, 200, 200, 200] 
dynamics_weight_decay : [2.5e-05, 5e-05, 7.5e-05, 7.5e-05, 0.0001] 
n_ensemble           : 7 
n_elites             : 5 
rollout_freq         : 1000 
rollout_batch_size   : 512 
rollout_length       : 1000 
penalty_coef         : 0.5 
num_samples          : 10 
model_retain_epochs  : 5 
real_ratio           : 0.05 
load_dynamics_path   : False 
epoch                : 3000 
step_per_epoch       : 1000 
eval_episodes        : 10 
batch_size           : 256 
lr_scheduler         : True 
